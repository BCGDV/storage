#!/bin/bash

# Notes:
#  - Please install "jq" package before using this driver.
WAIT_SLEEP_TIME_IN_SECONDS=2

if [ -e "$(dirname $0)/common.sh" ]; then
    source $(dirname $0)/common.sh
elif [ -e "$(dirname $0)/../common/common.sh" ]; then
    source $(dirname $0)/../common/common.sh
fi

get_meta_data() {
    EC2_AVAIL_ZONE=`curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone 2>/dev/null`
    EC2_REGION="`echo \"$EC2_AVAIL_ZONE\" | sed -e 's:\([0-9][0-9]*\)[a-z]*\$:\\1:'`"
    INSTANCE_ID=`curl http://169.254.169.254/latest/meta-data/instance-id 2>/dev/null`
}

wait_volume_transition() {
    local current_state=$1
    local start_state=$1
    local end_state=$2
    while [ "${current_state}" == "${start_state}" ]; do
        sleep ${WAIT_SLEEP_TIME_IN_SECONDS}
        local volumes=`aws ec2 describe-volumes --region ${EC2_REGION} --volume-ids ${VOLUME_ID} 2>&1`
        if [ $? -ne 0 ]; then
            print_error "Failed to describe volume ${VOLUME_ID}: ${volumes}"
        fi
        current_state=$(echo ${volumes} | jq -r '.Volumes[0].State')
    done
    if [ "${current_state}" != "${end_state}" ]; then
        print_error "Failed volume ${VOLUME_ID} transition, expected end state is: ${end_state}, got ${current_state}"
    fi

}

wait_volume_attaching() {
    local attach_state="attaching"
    while [ "$attach_state" == "attaching" ]; do
        sleep ${WAIT_SLEEP_TIME_IN_SECONDS}
        local volumes=`aws ec2 describe-volumes --region ${EC2_REGION} --volume-ids ${VOLUME_ID} 2>&1`
        if [ $? -ne 0 ]; then
            print_error "Failed to describe volume ${VOLUME_ID}: ${volumes}"
        fi
        attach_state=$(echo ${volumes} | jq -r '.Volumes[0].Attachments[0].State')
    done
    if [ "$attach_state" != "attached" ]; then
        print_error "Failed to attach volume ${VOLUME_ID}, final state is: ${attach_state}"
    fi
}

# TODO remove dead code unless we find a use for this
detach_if_attached(){
    local volumes=`aws ec2 describe-volumes --region ${EC2_REGION} --volume-ids ${VOLUME_ID} 2>&1`
    if [ $? -ne 0 ]; then
        print_error "Failed in attach: ${volumes}"
    fi

    # get attached instance id
    local attached_instance_id=$(echo ${volumes} | jq -r '.Volumes[0].Attachments[0].InstanceId')

    # check if volumeID is already attached to another instance
    if [ "${attached_instance_id}" != "null" ]; then
        local instance_status=`aws ec2 describe-instance-status --region ${EC2_REGION} --instance-ids ${attached_instance_id} 2>&1`
        if [ $? -ne 0 ]; then
            print_error "Failed in attach: ${instance_status}"
        fi

        # if currently attached instance VM is running, we can't detach, error out here
        local status=$(echo ${instance_status} | jq -r '.InstanceStatuses[0].InstanceState.Name')
        if [ "${status}" == "running" ] && [ "${attached_instance_id}" != "${INSTANCE_ID}" ]; then
            print_error "Failed: volume ${VOLUME_ID} is currently attached to a running instance ${attached_instance_id}, can not detach or re-use it"
        fi

        # detach it from currently attached instance VM
        local error=`aws ec2 detach-volume --region ${EC2_REGION} --volume-id ${VOLUME_ID} --instance-id ${attached_instance_id} 2>&1`
        if [ $? -ne 0 ]; then
            print_error "Failed to detach ebs volume-id ${VOLUME_ID} from instance-id ${attached_instance_id}. ${error}"
        fi
        wait_volume_transition "in-use" "available"
    fi
}

find_available_device_path() {
    local attachments=`aws ec2 describe-volumes --region ${EC2_REGION} --filters Name=attachment.instance-id,Values=${INSTANCE_ID} | jq -r '.Volumes[].Attachments[]'`
    if [ $? -ne 0 ]; then
        print_error "Failed to describe volume of instance-id ${INSTANCE_ID}. ${attachments}"
    fi

    local prefered_device_paths="fghijklmnop"
    for (( i=0; i<${#prefered_device_paths}; i++ )); do
        local current="/dev/sd${prefered_device_paths:$i:1}"
        local device=`echo ${attachments} | jq "select(.Device==\"${current}\") | .Device"`
        if [ "${device}" == "" ]; then
            echo ${current};
            break
        fi
    done
}

find_matching_linux_device_path() {
    while true; do
        local linux_device_path=`lsblk -o KNAME | sed -e 's/^/\/dev\//' | grep "$1"`
        if [ "${linux_device_path}" == "$1" ]; then
            echo ${linux_device_path}
            break
        fi

        local device_new_format="/dev/xvd""${1: -1}"
        local linux_device_path=`lsblk -o KNAME | sed -e 's/^/\/dev\//' | grep "${device_new_format}"`
        if [ "${linux_device_path}" == "${device_new_format}" ]; then
            echo ${linux_device_path}
            break
        fi

        sleep ${WAIT_SLEEP_TIME_IN_SECONDS}
    done
}

init()
{
    print_success
}

create() {
    if [ ! -z "${OPTS[volumeID]}" ]; then
        print_success
        exit 0
    fi

    if [ -z "${OPTS[name]}" ]; then
        print_error "name is required"
    fi

    if [ -z "${OPTS[size]}" ]; then
        print_error "size is required"
    fi

    local type=${OPTS[volumeType]}
    local iops_option=""
    if [ "${type}" == "io1" ]; then
        if [ -z "${OPTS[iops]}" ]; then
            print_error "iops is required"
        else
            iops_option="--iops ${OPTS[iops]}"
        fi
    fi

    local type_option=""
    if [ ! -z "${type}" ]; then
        type_option="--volume-type ${type}"
    fi

    unset_aws_credentials_env

    get_meta_data

    # create a EBS volume using aws-cli
    local volume=`aws ec2 create-volume --region ${EC2_REGION} --size ${OPTS[size]} --availability-zone ${EC2_AVAIL_ZONE} ${type_option} ${iops_option} 2>&1`
    if [ $? -ne 0 ]; then
        # now volume is the error message
        print_error "Failed in create: ${volume}"
    fi
    VOLUME_ID=$(echo ${volume} | jq -r '.VolumeId')

    wait_volume_transition "creating" "available"

    # tag the newly created volume
    local error=`aws ec2 create-tags --region ${EC2_REGION} --resources ${VOLUME_ID} --tags Key=Name,Value=${OPTS[name]} 2>&1`
    if [ $? -ne 0 ]; then
        print_error "Failed in create: create-tags for volume ${VOLUME_ID} Key=Name,Value=${OPTS[name]} failed. ${error}"
    fi

    print_options created true volumeID ${VOLUME_ID}
}

delete() {
    if [ -z "${OPTS[created]}" ]; then
        print_success
        exit 0
    fi

    if [ -z "${OPTS[volumeID]}" ]; then
        print_error "volumeID is required"
    fi

    VOLUME_ID=${OPTS[volumeID]}

    unset_aws_credentials_env

    get_meta_data

    local volumes=`aws ec2 describe-volumes --region ${EC2_REGION} --volume-ids ${VOLUME_ID} 2>&1`
    if [ $? -ne 0 ]; then
        print_error "Failed to describe volume ${VOLUME_ID}: ${volumes}"
    fi

    current_state=$(echo ${volumes} | jq -r '.Volumes[0].State')
    if [ "${current_state}" != "available" ]; then
        print_error "Failed to delete volume ${VOLUME_ID}, current state: ${current_state} is not available"
    fi

    local error=`aws ec2 delete-volume --region ${EC2_REGION} --volume-id ${VOLUME_ID} 2>&1`
    if [ $? -ne 0 ]; then
        print_error "Failed to delete volume ${VOLUME_ID}. ${error}"
    fi

    print_success
}

attach() {
    print_not_supported
}

detach() {
    local aws_device_path="/dev/sd""${DEVICE: -1}"

    unset_aws_credentials_env

    get_meta_data

    # from device, get VOLUME_ID
    local attachments=`aws ec2 describe-volumes --region ${EC2_REGION} --filters Name=attachment.instance-id,Values=${INSTANCE_ID} | jq -r '.Volumes[].Attachments[]'`
    if [ $? -ne 0 ]; then
        print_error "Failed to describe volume of instance-id ${INSTANCE_ID}. ${attachments}"
    fi

    VOLUME_ID=`echo ${attachments} | jq "select(.Device==\"${aws_device_path}\") | .VolumeId"`
    VOLUME_ID=`echo ${VOLUME_ID=} | sed -e 's/^"//' -e 's/"$//'`

    # no such device attached
    if [ -z "${VOLUME_ID}" ]; then
        print_success
        exit 0
    fi

    local error=`aws ec2 detach-volume --region ${EC2_REGION} --volume-id ${VOLUME_ID} 2>&1`
    if [ $? -ne 0 ]; then
        print_error "Failed to detach ebs volume-id ${VOLUME_ID} from current instance. ${error}"
    fi
    wait_volume_transition "in-use" "available"

    print_success
}

mountdest() {
    if [ -z "${OPTS[volumeID]}" ]; then
        print_error "volumeID is required"
    fi

    VOLUME_ID=${OPTS[volumeID]}

    unset_aws_credentials_env

    get_meta_data

    local device_path=$(find_available_device_path)

    local error=`aws ec2 attach-volume --region ${EC2_REGION} --volume-id ${VOLUME_ID} --instance-id ${INSTANCE_ID} --device ${device_path} 2>&1`
    if [ $? -ne 0 ]; then
        print_error "Failed to attach ebs volume-id ${VOLUME_ID} as device:${device_path} to aws instance-id ${INSTANCE_ID}. ${error}"
    fi

    wait_volume_attaching

    # need to return real linux device path, it could be in a different format "sdf" => "xvdf"
    local linux_device_path=$(find_matching_linux_device_path "${device_path}")

    local error=`mkfs.ext4 -F $linux_device_path 2>&1`
    if [ $? -ne 0 ]; then
        print_error $error
    fi

    local error=`mount $linux_device_path $MNT_DEST 2>&1`
    if [ $? -ne 0 ]; then
        print_error $error
    fi

    print_success
}

unmount() {
    set +e
    result=$(umount $MNT_DEST 2>&1)
    exitcode=$?
    if [ "$exitcode" != "0" ]; then
        if [ "$(echo $result | grep 'not mounted')" ]; then
            print_success not mounted
        elif [ "$(echo $result | grep 'mountpoint not found')" ]; then
            print_success not found
        else
            print_error $result
        fi
    fi
    print_success unmounted
}

# Every script must call main as such
main "$@"